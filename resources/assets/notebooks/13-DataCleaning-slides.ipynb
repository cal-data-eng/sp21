{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Transformation 3: Data Cleaning\n",
    "1. Hygiene for Cleaning\n",
    "1. Outlier detection and handling\n",
    "1. Imputation (missing values)\n",
    "1. String proximity and Entity Resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hygiene for Data \"Cleaning\"\n",
    "The very term \"data cleaning\" is problematic.\n",
    "- Presumes that the raw data is \"truly\" dirty. We don't know that.\n",
    "- We are inherently imposing a model over the data\n",
    "- Let us not impose our will on the raw data!\n",
    "\n",
    "Better to think of today's tasks as Transformation functions!\n",
    "- With recorded input and output\n",
    "- And the \"lineage\" of how the output is computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Embrace some simple metadata \n",
    "If you are transforming just one column\n",
    "- Keep the original column\n",
    "- Add a derived column one to the right if possible\n",
    "- Name the derived column something meaningful\n",
    "\n",
    "If you are transforming much of a data set\n",
    "- Create a new derived dataset and store it \"near\" the original\n",
    "  - filesystem directory, database schema, git repo, etc\n",
    "- Name the derived dataset something that hints at the lineage\n",
    "\n",
    "In all cases, keep the transformation code!\n",
    "- Manage/version it as you would source code\n",
    "- Document it as you would source code\n",
    "- Hopefully in the same repository/toolchain as the data\n",
    "\n",
    "We'll talk more about metadata and data lineage later in the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outlier Detection and Handling\n",
    "What is an \"outlier\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normal Distribution\n",
    "Center and dispersion (spread).\n",
    "- Center: mean\n",
    "- Dispersion: stddev\n",
    "\n",
    "Outliers are values with \"high\" spread from the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Fig size\n",
    "plt.rcParams[\"figure.figsize\"]=12,8\n",
    "\n",
    "## replace the database connection with a database of your own!\n",
    "%reload_ext sql\n",
    "%sql postgresql://jmh@localhost:5432/jmh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Normal Distributions are nice\n",
    "Let's set up a good old-fashioned univariate Gaussian (normal) in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%sql DROP TABLE IF EXISTS observations CASCADE;\n",
    "%sql CREATE TABLE observations AS \\\n",
    "     SELECT normal_rand AS x FROM normal_rand(1000, 50, 5);\n",
    "\n",
    "results = %sql SELECT x FROM observations\n",
    "sns.displot(results.dict(), fill=True, kde=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Detecting Gaussian Outliers\n",
    "One rule of thumb: outliers are 2 stddevs ($2\\sigma$) from the mean on either side.\n",
    "- Based on the normal distribution, 2 stddev's is about 95% of the data\n",
    "- So outliers are below p2.5 and above p97.5\n",
    "- We could of course pick $3\\sigma$ (99.7% of the data) or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW normal_outliers AS\n",
    " WITH bounds AS (\n",
    " SELECT avg(x) - 2*stddev(x) AS lo, avg(x) + 2*stddev(x) AS hi\n",
    "   FROM observations\n",
    " )\n",
    " SELECT x FROM observations o, bounds b\n",
    "  WHERE x NOT BETWEEN b.lo AND b.hi;\n",
    "    \n",
    "SELECT * FROM normal_outliers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Handling Gaussian Outliers\n",
    "- One option is simply to *delete* the outlying values from consideration. \n",
    "- Now let's look at the data with and without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "results = %sql SELECT x, 'original' as label FROM observations \\\n",
    "                UNION ALL \\\n",
    "               SELECT x, 'cleaned' FROM observations \\\n",
    "                WHERE x NOT IN (SELECT * FROM normal_outliers)\n",
    "sns.displot(results.dict(), x=\"x\", kind='hist', hue='label', kde=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Non-Gaussian Data\n",
    "What if you corrupt just one value to be very large? \n",
    "- Right-biased: not a normal distribution anymore! \n",
    "- Messes up \"textbook\" definitions of center and dispersion!\n",
    "    - They assumed a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## corrupt one value\n",
    "%sql UPDATE observations SET x = x*10 \\\n",
    "      WHERE x = (SELECT MAX(x) FROM OBSERVATIONS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What Happened??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "results = %sql SELECT x, 'orig' as label FROM observations \\\n",
    "                UNION ALL \\\n",
    "               SELECT x, 'cleaned' FROM observations \\\n",
    "                WHERE x NOT IN (SELECT * FROM normal_outliers)\n",
    "sns.displot(results.dict(), x=\"x\", kind='hist', hue='label', kde=True, bins=20, rug=True)\n",
    "%sql select x from normal_outliers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Masking\n",
    "The $10x$ value is *masking* our earlier outliers\n",
    "- Even the one on the left!\n",
    "- We can mask any outlier we please with an even bigger outlier!\n",
    "\n",
    "Gaussian definitions of \"center\" and \"dispersion\" are not **robust**\n",
    "  - 1 value can drag the mean and stddev *as far as you want*!\n",
    "\n",
    "Robust measures should tolerate some corruption\n",
    "  - after all, the whole point is to handle dirty data!\n",
    "  - we'll define robustness formally shortly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Trimming: Percentile Outliers\n",
    "- Suppose we define the outliers by order statistics (percentiles)\n",
    "- *Trimming*: dropping outliers based on order statistics\n",
    "- E.g. a \"1% trimmed distribution\" drops the 1% on either end\n",
    "  - p1 and p99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%sql DROP TABLE observations CASCADE;\n",
    "%sql CREATE TABLE observations AS SELECT normal_rand AS x FROM normal_rand(1000, 50, 5);\n",
    "\n",
    "results = %sql SELECT x FROM observations\n",
    "sns.displot(results.dict(), fill=True, kde=True, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW p1p99 AS\n",
    "SELECT percentile_cont(.01) WITHIN GROUP (ORDER BY x) AS p1,\n",
    "           percentile_cont(.99) WITHIN GROUP (ORDER BY x) AS p99\n",
    "      FROM observations;\n",
    "SELECT * FROM p1p99;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW trimmed_observations AS\n",
    "SELECT o.x, 'trimmed' AS label\n",
    "  FROM observations o, p1p99 p\n",
    " WHERE o.x BETWEEN p.p1 AND p.p99\n",
    "UNION ALL\n",
    "SELECT o.x, 'original' AS label\n",
    "  FROM observations o;\n",
    "\n",
    "CREATE OR REPLACE VIEW trimmed_outliers AS\n",
    "SELECT o.*\n",
    "  FROM observations o, p1p99 p\n",
    " WHERE o.x NOT BETWEEN p.p1 AND p.p99;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "results = %sql SELECT * from trimmed_observations\n",
    "sns.displot(results.dict(), x=\"x\", kind='hist', hue='label', kde=True, bins=20)\n",
    "\n",
    "results = %sql SELECT x from trimmed_outliers\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if you corrupt just one value to be very large? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%sql UPDATE observations SET x = x*10 WHERE x = (SELECT MAX(x) FROM observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# WHERE x < 500\n",
    "results = %sql SELECT * FROM trimmed_observations\n",
    "sns.displot(results.dict(), x=\"x\", kind='hist', hue='label', kde=True, bins=20)\n",
    "%sql SELECT * FROM trimmed_outliers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Is Trimming More Robust than Stddev-Based?\n",
    "Minor masking on the right, but not nearly as bad.\n",
    "\n",
    "- Maybe we should have trimmed less? More? How much? Hmmm...\n",
    "  - Seems like it should depend on the data!\n",
    "- Before we answer that, one more standard outlier handling scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Winsorizing\n",
    "- Trimming:\n",
    "  - dropped the $k\\%$ tails \n",
    "- Winsorizing:\n",
    "  - *replace* those values with the $k$-percentile value.\n",
    "  - $k\\%$ tails contain the same repeated value\n",
    "- This preserves the probability density of the tails.\n",
    "  - usually not a big difference from trimming\n",
    "    - mostly seen in the stddev, not the mean\n",
    "  - Winsorize preferred to Trimming if something downstream forbids NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW winsorized_observations AS\n",
    "SELECT CASE WHEN o.x BETWEEN p.p1 AND p.p99 THEN o.x\n",
    "            WHEN o.x < p.p1 THEN p.p1\n",
    "            WHEN o.x > p.p99 THEN p.p99\n",
    "        END AS x,\n",
    "      'winsorized' AS label\n",
    "  FROM observations o, p1p99 p\n",
    "UNION ALL\n",
    "SELECT o.x, 'original' AS label\n",
    "  FROM observations o;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Winsorized distribution against the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# WHERE x < 500\n",
    "results = %sql SELECT * from winsorized_observations\n",
    "sns.displot(results.dict(), x=\"x\", kind='hist', hue='label', kde=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Seems close to trimmed. Let's look more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT 'orig' AS distribution, min(x),\n",
    "       percentile_disc(.25) WITHIN GROUP (ORDER BY x) as p25,\n",
    "       percentile_disc(.50) WITHIN GROUP (ORDER BY x) as median,\n",
    "       percentile_disc(.75) WITHIN GROUP (ORDER BY x) as p75,\n",
    "       max(x), avg(x), stddev(x), count(x) \n",
    "       FROM observations\n",
    "UNION ALL\n",
    "SELECT 'winsorized', min(x),\n",
    "       percentile_disc(.25) WITHIN GROUP (ORDER BY x) as p25,\n",
    "       percentile_disc(.50) WITHIN GROUP (ORDER BY x) as median,\n",
    "       percentile_disc(.75) WITHIN GROUP (ORDER BY x) as p75,\n",
    "       max(x), avg(x), stddev(x), count(x) \n",
    "       FROM winsorized_observations WHERE label = 'winsorized'\n",
    "UNION ALL \n",
    "SELECT 'trimmed', min(x),\n",
    "       percentile_disc(.25) WITHIN GROUP (ORDER BY x) as p25,\n",
    "       percentile_disc(.50) WITHIN GROUP (ORDER BY x) as median,\n",
    "       percentile_disc(.75) WITHIN GROUP (ORDER BY x) as p75,\n",
    "       max(x), avg(x), stddev(x), count(x) \n",
    "       FROM trimmed_observations WHERE label = 'trimmed';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robustness\n",
    "Robustness is a worst-case analysis. So we think in terms of an *adversary*.\n",
    "\n",
    "Suppose the adversary could \"corrupt\" data values arbitrarily. What does that do to an \"estimator\" (i.e. an aggregate like mean, stddev, etc.)?\n",
    "- Defn: **Breakdown Point** of an estimator\n",
    "  - smallest fraction of values the adversary must corrupt to return an *arbitrary result*\n",
    "  - i.e. to change the center and spread to *whatever it wants*?\n",
    "  - Depends on the definition of center and spread!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the breakdown point of the $1\\%$ trimmed mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What $k\\%$ gives us maximum robustness via trimming? \n",
    "- Could we do any better with another scheme?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Robust Can You Be?\n",
    "The median of any distribution is maximally robust\n",
    "- up to $50\\%$ corruption of the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robust Estimators of a Distribution\n",
    "- Center: Median\n",
    "- Dispersion: the Median Absolute Deviation (MAD)\n",
    "\n",
    "Given dataset $X$ with $\\tilde X = \\mbox{median}(X)$, we define the MAD as:\n",
    "$$MAD(X) = \\mbox{median}(|X_i - \\tilde X|)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The Median and MAD are both maximally robust!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- percentile_disc returns an actual data value near the percentile\n",
    "-- percentile_cont returns an interpolated value at the percentile\n",
    "CREATE OR REPLACE VIEW median AS\n",
    "(SELECT percentile_disc(0.5) WITHIN GROUP (ORDER BY x) as median\n",
    "  FROM observations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW mad AS\n",
    "WITH\n",
    "absdevs AS\n",
    "(SELECT abs(x - median) as d\n",
    "   FROM observations, median)\n",
    "SELECT percentile_disc(0.5) WITHIN GROUP (ORDER BY d) as mad\n",
    "  FROM absdevs;\n",
    "    \n",
    "SELECT median, mad\n",
    "  FROM median, mad;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Robust Centers/Dispersion\n",
    "You'll commonly see people use:\n",
    "- center: k% trimmed mean\n",
    "- center: k% winsorized mean\n",
    "- dispersion: Interquartile Range (IQR: p75 - p25)\n",
    "\n",
    "Recall the Tukey numbers for assessing univariate numerics:\n",
    "- min, p25, median, p75, max: robust center/dispersion\n",
    "- center: median\n",
    "- spread: informed by min/max *and* IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A Robust Outlier Metric: Hampel x84\n",
    "Quartiles just a rule of thumb, and ignore dispersion.\n",
    "\n",
    "Back to the earlier question: How much should we trim or winsorize?\n",
    "- Let's use our intuition from the normal distribution.\n",
    "- E.g. \"$2\\sigma$ from the mean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hampel X84: Intuition from Normal\n",
    "\"Translate\" normal estimators to robust center/dispersion.\n",
    "\n",
    "- assume a standard normal distribution (mean 0 stddev 1)\n",
    "- convert standard deviation to MADs\n",
    "    - in this standard case, 1 stddev = 1.4826 MADs!\n",
    "    - (Challenge: write Python or SQL to test that!)\n",
    "\n",
    "Hampel x84: define outliers as $k*1.4826$ MADs from the median!\n",
    "- benefit: outliers defined by (robust) dispersion of the data\n",
    "- as opposed to IQR, etc which ignores dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Redoing our outliers with Hampel x84\n",
    "Let's find/trim outliers $2*1.4826$ from the median!\n",
    "- This is just like the order statistics above\n",
    "- But also like $2\\sigma$\n",
    "  - trims based on a (robust) metric of spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE VIEW hampelx84x2_observations AS\n",
    "SELECT o.x,\n",
    "      'hampelx84x2' AS label\n",
    "  FROM observations o, median, mad\n",
    " WHERE o.x BETWEEN (median-2*1.4826*mad) AND (median+2*1.4826*mad)\n",
    "UNION ALL\n",
    "SELECT o.x, 'orig' AS label\n",
    "  FROM observations o;\n",
    "\n",
    "CREATE OR REPLACE VIEW Hampel84x2_outliers AS\n",
    "SELECT x\n",
    "  FROM observations o, median, mad\n",
    " WHERE x NOT BETWEEN (median - 2*1.4826*mad) AND (median + 2*1.4826*mad);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# WHERE x < 500\n",
    "results = %sql SELECT * FROM hampelx84x2_observations\n",
    "sns.displot(results.dict(), x=\"x\", kind='hist', hue='label', kde=True, bins=20)\n",
    "%sql SELECT * FROM Hampel84x2_outliers;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model-Based Outlier Detection\n",
    "Up to now we've been cleaning stored data values. A lot of outlier detection discussion is around model fitting.\n",
    "\n",
    "Assume you're fitting a model to your data.\n",
    "- E.g. linear regression\n",
    "\n",
    "Q: Which data points are \"outliers\" with respect to the model.\n",
    "- \"Surprising\" values\n",
    "- Use our outlier metrics, but apply to the *model residuals*\n",
    "    - E.g. L2 distance between actual value and predicted\n",
    "- Assumption: residuals of your model are normally distributed\n",
    "\n",
    "Anything further down this path is well into the realm of data analysis, not engineering.\n",
    "- So we'll stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outliers: Summing up\n",
    "Detection: Center and Spread\n",
    "- The normal distribution gives nice intuition, but not robust\n",
    "- Robustness: want high breakdown point (at most 50%)!\n",
    "- Order statistics like percentiles are robust\n",
    "    - But don't take dispersion into account\n",
    "- Median and MAD are robust estimators of center and dispersion, respectively\n",
    "- Hampel X84: robust outlier metric, considers dispersion\n",
    "\n",
    "Outlier Handling:\n",
    "- Trimming\n",
    "- Winsorizing\n",
    "- Watch your hygiene!\n",
    "  - Keep the raw data, close by\n",
    "  - Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Imputation\n",
    "Sometimes when data is missing, we fill in \"likely\" values. Why?\n",
    "- Missing data can lead to bias\n",
    "- Some downstream operators won't tolerate missing data\n",
    "    - E.g. a stats package that needs a dense tensor\n",
    "\n",
    "Strategies for defining what's \"likely\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Default values for a column\n",
    "    - Typically an aggregate of the column\n",
    "    - E.g. the center (mean/median)\n",
    "- Correlation across columns (e.g. $P(\\mbox{elevation} | \\mbox{latitude})$)\n",
    "- Sampled from a model (possibly trained on other data)\n",
    "- Interpolation across (ordered) rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Choice of Imputation Methods\n",
    "What is a *good* imputation scheme for your setting?\n",
    "- It depends!\n",
    "- This is part of the art of statistics\n",
    "- We will not offer prescriptions here\n",
    "   - Focus on *how* rather than *what*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For purposes of illustration, let's introduce some missing values into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## replace the database connection with a database of your own!\n",
    "%reload_ext sql\n",
    "%sql postgresql://jmh@localhost:5432/gnis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT setseed(0.12345);\n",
    "DROP TABLE IF EXISTS holey CASCADE;\n",
    "CREATE TABLE holey AS \n",
    "SELECT feature_id, feature_name, feature_class, state, county_name, \n",
    "       primary_latitude_dec, primary_longitude_dec, \n",
    "       CASE WHEN random() > 0.9 THEN NULL\n",
    "            ELSE elevation_meters\n",
    "        END AS elevation_meters\n",
    "  FROM national;\n",
    "SELECT count(elevation_meters)::float / count(*) FROM holey;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Default Value Imputation in SQL\n",
    "Two pass: i.e. an aggregate CTE followed by a query. E.g. mean imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH elevavg AS (SELECT avg(elevation_meters) FROM holey)\n",
    "SELECT h.*, \n",
    "       CASE WHEN h.elevation_meters IS NOT NULL THEN h.elevation_meters\n",
    "            ELSE e.avg\n",
    "        END AS imputed_elevation_meters\n",
    "  FROM holey h, elevavg e\n",
    " LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Correlation Across Columns\n",
    "Given a correlation model, applying it is just a scalar function!\n",
    "- E.g. linear regression from longitude to elevation_meters?\n",
    "- Just apply slope and intercept to longitude!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here we'll train the model in SQL just for fun\n",
    "result = %sql SELECT regr_slope(elevation_meters, primary_longitude_dec), regr_intercept(elevation_meters, primary_longitude_dec) FROM holey\n",
    "slope, intercept = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *,\n",
    "       CASE WHEN elevation_meters IS NOT NULL THEN elevation_meters\n",
    "            ELSE :slope*primary_longitude_dec + :intercept\n",
    "        END AS imputed_elevation_meters\n",
    "  FROM holey\n",
    " LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "Compare the original data and the imputed data\n",
    "- Compute the residuals\n",
    "- What is the distribution of residuals?\n",
    "- What do the outliers of the residuals look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model-Based Interpolation\n",
    "Like the previous case, just trained on different data in advance.\n",
    "\n",
    "Call a scalar function taking a model prediction function, passing parameters from the values in the row:\n",
    "```\n",
    "SELECT *,\n",
    "       CASE WHEN column IS NOT NULL THEN column\n",
    "            ELSE model_predict(<constants>, <columns>)\n",
    "        END\n",
    "  FROM table;\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Correlation across ordered rows\n",
    "This typically involves a window function. A simple example is just to \"fill down\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- The following doesn't work in PostgreSQL!\n",
    "WITH buggy AS (\n",
    "SELECT *,\n",
    "       CASE WHEN elevation_meters IS NOT NULL THEN elevation_meters\n",
    "            ELSE lag(elevation_meters, 1)\n",
    "                 OVER (ORDER BY feature_id)\n",
    "        END AS imputed_elevation_meters\n",
    "  FROM holey\n",
    ")\n",
    "SELECT * FROM buggy LIMIT 500;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using a UDA to Simulate lag(...) IGNORE NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Here's a UDA fix from\n",
    "-- https://stackoverflow.com/questions/18987791/how-do-i-efficiently-select-the-previous-non-null-value\n",
    "CREATE OR REPLACE FUNCTION coalesce_agg_sfunc(state anyelement, value anyelement) RETURNS anyelement AS\n",
    "$$\n",
    "    SELECT coalesce(value, state);\n",
    "$$ LANGUAGE SQL;\n",
    "\n",
    "CREATE AGGREGATE coalesce_agg(anyelement) (\n",
    "    SFUNC = coalesce_agg_sfunc,\n",
    "    STYPE  = anyelement);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Redoing our Fill Down Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Fixed to handle repeated NULLs\n",
    "WITH fixed AS (\n",
    "SELECT *,\n",
    "       coalesce_agg(elevation_meters) OVER (order by feature_id) AS imputed_elevation_meters\n",
    "  FROM holey\n",
    ")\n",
    "SELECT * FROM fixed LIMIT 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Test for NULLs\n",
    "WITH fixed AS (\n",
    "SELECT *,\n",
    "       coalesce_agg(elevation_meters) OVER (order by feature_id) AS imputed_elevation_meters\n",
    "  FROM holey\n",
    ")\n",
    "SELECT count(*) FROM fixed WHERE imputed_elevation_meters IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## General Interpolation Across Ordered Rows\n",
    "This turns out to be relatively tricky/expensive: requires multiple passes over window aggs!\n",
    "\n",
    "Our goal is to be interpolating across \"runs\" of NULLs.\n",
    "- To interpolate, we need to label every row with\n",
    "  - a unique \"run\" number (`run`) for each value and its subsequent NULLs\n",
    "  - the initial, non-NULL value in the run (`run_start`)\n",
    "  - this row's index in the run (`run_rank`)\n",
    "  - the total number of rows in this run (`run_size`)\n",
    "  - the next non-NULL value in order (`run_end`)\n",
    "\n",
    "With this, we can interpolate via whatever scalar math we like (e.g. linear).\n",
    "\n",
    "# PUT PICTURE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Strategy for Interpolation\n",
    "A 3-pass algorithm:\n",
    "1. Forward: \n",
    "  - compute `run`\n",
    "  - propagate `run_start`\n",
    "  - get `nextval` into last row of run\n",
    "2. Backward, given `run` partitions: \n",
    "  - compute `run_size`\n",
    "  - computer `run_rank`\n",
    "  - propagate `run_end` from `nextval`\n",
    "3. The final query uses scalars to interpolate\n",
    "\n",
    "Can you do better? If you don't use SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- 1. Forward assign \"run\" numbers to rows, propagate val, get nextval\n",
    "CREATE OR REPLACE VIEW forward AS\n",
    "SELECT *,\n",
    "       SUM(CASE WHEN elevation_meters IS NULL THEN 0 ELSE 1 END) \n",
    "         OVER (ORDER BY feature_id) AS run,\n",
    "       coalesce_agg(elevation_meters) OVER (ORDER BY feature_id) AS run_start,\n",
    "       CASE WHEN elevation_meters IS NULL \n",
    "              THEN lead(elevation_meters, 1) OVER (ORDER BY feature_id)\n",
    "            ELSE NULL\n",
    "             END AS nextval\n",
    "  FROM holey;\n",
    "\n",
    "SELECT * FROM forward\n",
    " LIMIT 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- 2. Backward: assign run_end, run_size, run_rank\n",
    "CREATE OR REPLACE VIEW backward AS\n",
    "SELECT *,\n",
    "       CASE WHEN elevation_meters IS NOT NULL THEN elevation_meters\n",
    "            ELSE coalesce_agg(nextval) OVER (PARTITION BY run ORDER BY feature_id DESC)\n",
    "        END AS run_end,\n",
    "       count(*) OVER (PARTITION BY run) AS run_size,\n",
    "       1 + feature_id - (min(feature_id) OVER (PARTITION BY run)) AS run_rank\n",
    "  FROM fw;\n",
    "\n",
    "SELECT * from backward LIMIT 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- 3. Simple scalar pass\n",
    "CREATE OR REPLACE VIEW final AS\n",
    "SELECT *, \n",
    "       run_start + (run_rank-1)*((run_end-run_start)/(run_size))\n",
    "         AS interpolated\n",
    "  FROM backward;\n",
    "\n",
    "SELECT * FROM final\n",
    "LIMIT 500;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How well did PostgreSQL do? Two sorts! Could you do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%sql EXPLAIN SELECT * from bw LIMIT 500;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Final Notes on Imputation\n",
    "- What we've discussed is standard *single imputation*\n",
    "- There are fancier statistical methods even [on Wikipedia](https://en.wikipedia.org/wiki/Imputation_(statistics)):\n",
    "  - E.g. *Multiple imputation* averages across multiple imputed datasets\n",
    "- Getting fancier may require (even) more query gymnastics!\n",
    "- You've seen enough to be dangerous!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Entity Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distance Functions on Strings\n",
    "Which is more likely:\n",
    "- \"Aditya\" $\\rightarrow$ \"Aditi\"\n",
    "- \"Aditya\" $\\rightarrow$ \"Adversary\n",
    "\n",
    "We want a notion of string \"distance\". There are many in the literature\n",
    "- [Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance): number of single-character edits\n",
    "    - Edits include insert, delete or substitute a character\n",
    "- [Jaro-Winkler](https://www.postgresql.org/docs/current/fuzzystrmatch.html): another edit distance, favors similar prefixes\n",
    "- Sound indexes: [Soundex](https://en.wikipedia.org/wiki/Soundex)/[Metaphone](https://en.wikipedia.org/wiki/Metaphone)/[Double Metaphone](https://en.wikipedia.org/wiki/Metaphone#Double_Metaphone)\n",
    "\n",
    "You can get [Postgres Functions](https://www.postgresql.org/docs/current/fuzzystrmatch.html) for Levenshtein, Soundex, Metaphone and Double Metaphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT levenshtein('Aditya', 'Aditi'),\n",
    "       soundex('Aditya') AS soundex1, \n",
    "       soundex('Aditi') AS soundex2,\n",
    "       metaphone('Aditya', 10) AS metaphone1, \n",
    "       metaphone('Aditi', 10) AS metaphone2,\n",
    "       dmetaphone('Aditya') AS dmetaphone1,\n",
    "       dmetaphone_alt('Aditya') AS dmetaphone_alt1,\n",
    "       dmetaphone('Aditi') AS dmetaphone2,\n",
    "       dmetaphone_alt('Aditi') AS dmetaphone_alt2\n",
    "UNION ALL\n",
    "SELECT levenshtein('Aditya', 'Adversary'),\n",
    "       soundex('Aditya'), soundex('Adversary'),\n",
    "       metaphone('Aditya', 10), metaphone('Adversary', 10),\n",
    "       dmetaphone('Aditya'), dmetaphone_alt('Adversary'),\n",
    "       dmetaphone('Aditya'), dmetaphone_alt('Adversary')\n",
    "UNION ALL\n",
    "SELECT levenshtein('Joe', 'Joel'),\n",
    "       soundex('Joe'), soundex('Joel'),\n",
    "       metaphone('Joe', 10), metaphone('Joel', 10),\n",
    "       dmetaphone('Joe'), dmetaphone_alt('Joel'),\n",
    "       dmetaphone('Joe'), dmetaphone_alt('Joel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Entity Resolution\n",
    "A.k.a. Record Linkage, Data Matching, Deduplication, Standardization\n",
    "\n",
    "Suppose I have a column of product names.\n",
    "- Might different names represent the same real-world \"entity\"?\n",
    "\n",
    "Suppose I have a table of product tuples\n",
    "- Might different tuples represent the same real-world \"entity\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simple Case: Matching to Reference Data\n",
    "Given noisy input data $D$, and a curated reference set $R$.\n",
    "Map each item in $D$ to the best match in $R$ (or NULL).\n",
    "\n",
    "Example: Suppose you have a curated table `Products`. \n",
    "- You then receive sales data from a subsidiary\n",
    "- You want to match their product names to `Products`:\n",
    "- Proximity join!\n",
    "```\n",
    "SELECT *\n",
    "  FROM new_sales, products\n",
    " WHERE levenshtein(new_sales.product_name, products.name) < 5;\n",
    "```\n",
    "This cross join will be nightmarishly slow!\n",
    "- How slow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matching to Reference Data, Cont.\n",
    "Desired: first-pass filter \n",
    "- Use a text-search index on `products.name`\n",
    "- Only compute levenshtein distance on top $k$ matches for each\n",
    "\n",
    "Many systems provide this\n",
    "- E.g. external text indexing via Elastic or the like\n",
    "    - Have to keep in sync with other databases\n",
    "- E.g. in-database text indexing like Postgres' [Gin indexes](https://www.postgresql.org/docs/current/gin-intro.html)\n",
    "    - Stays automatically in sync\n",
    "    - Expensive on insertion\n",
    "    - Not super fast compared to a custom text index\n",
    "\n",
    "How does all that work?\n",
    "- We'll return to this topic when we discuss unstructured data and search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entity Resolution (Without Reference Data)\n",
    "Given input data $D$, partition into equivalence classes corresponding to distinct real-world entities.\n",
    "\n",
    "Clearly a heuristic problem!\n",
    "\n",
    "- Approach: as before, need to filter first!\n",
    "  - *Blocking*: group the data into (possibly overlapping) subsets\n",
    "  - *Matching*: Within each block, try to match up entities\n",
    "      - *Pairwise*: Create weighted edges via distance function between pairs in a block\n",
    "      - *Transitive*: Partition the resulting graph into *clusters*\n",
    "          - \"close\" nodes should be in the same cluster\n",
    "          - \"distant\" nodes in different clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entity Resolution 1: Blocking\n",
    "Idea:\n",
    "  1. Extract one or more \"blocking keys\" from each entity\n",
    "  2. Form blocks out of entities that have same/similar blocking keys\n",
    "  \n",
    "Standard approach: partiton by some column(s)\n",
    "  - Each group is a distinct block\n",
    "  - Each item is in only one block, like GROUP BY\n",
    "\n",
    "Fast, easy to understand/engineer.\n",
    "  - Can prep data for this by derive blocking \"features\"\n",
    "    - E.g. initials/acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Blocking Schemes\n",
    "*Many* heuristics here. We'll learn $q$-gram blocking, mostly because $q$-grams are handy to know about anyhow.\n",
    "\n",
    "Definition: the **$q$-grams** of a string are the substrings of length $q$.\n",
    "  - E.g. trigrams of `(picasso, pablo)` are {'pic', 'ica', 'cas', 'ass', 'sso', 'so\\\\$', 'o\\\\$p', '\\\\$pa', 'pab', 'abl', 'blo', 'lo\\\\$'}\n",
    "  - Handy for computing string-similarity\n",
    "  - Handy for indexing: build a B-tree on $q$-grams for finding misspellings!\n",
    " \n",
    "One block per distinct $q$-gram in your dataset. How to choose $q$? Heuristic.\n",
    "  - $q$ too small? Most entities are in most blocks (slow! bad *precision*)\n",
    "  - $q$ too big? Entities that should be together are not (bad *recall*)\n",
    "  - Can extend to require matches on multiple $q$-grams\n",
    "    - blows up the # of blocks, but each block smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entity Resolution 2a: Distance Metrics for Matching\n",
    "- Univariate distance\n",
    "  - String distances as above are limited\n",
    "  - Abbreviations: IBM = International Business Machines\n",
    "  - Synonyms: Eggplant = Aubergine\n",
    "  - Knowledge-base hierarchies: *tangerine* is closer to *orange* than it is to *apple*\n",
    "  - Upshot: there are many many heuristics in this domain!\n",
    "- Multivariate (tuple) distance\n",
    "  - `(Davis, Miles, 1950)`, `(Marsalis, Wynton, 1986)`, `(Davis, Miles, 1986)`\n",
    "  - `(Kelloggs, Corn Flakes, 1950)`, `(Post, Shredded Wheat, 1950)`, `(Kelloggs, Corn Flakes, 2021)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Distance Metric Practicalities\n",
    "  - In the absence of a supervised model, combine simple distance functions & weights\n",
    "  - If you can get labeled data, train a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entity Resolution 2b: Clustering\n",
    "Given: a block of size $b$ with $b^2$ pairwise distances\n",
    "  - This is a fully-connected graph on $b$ nodes\n",
    "  - Again we can heuristically threshold\n",
    "    - 2 nodes close enough? Declared \"same\"\n",
    "    - 2 nodes far enough? Prune the edge\n",
    "    - Others: uncertain\n",
    "  - Reasonable clustering goal (Correlation Clustering):\n",
    "    - maximize the sum of intra-cluster edge weights\n",
    "    - minimize the sum of inter-cluster edge weights\n",
    "    - This is *intractable* (NP-Hard) in theory\n",
    "    - Various approximation algorithms will do a good job\n",
    "        - E.g. [Clustering Aggregation](http://cs-people.bu.edu/evimaria/cs565/aggregated-journal.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Entity Resolution in Practice\n",
    "\n",
    "Do-it-yourself:\n",
    "1. Pick a blocking technique\n",
    "    - Easy: GROUP BY on some columns\n",
    "2. Pick some distance metrics\n",
    "3. Call a clustering library\n",
    "\n",
    "Use a product:\n",
    "- [open source](https://www.biggorilla.org/software_cat/entity-matching/index.html) not so strong at scale\n",
    "    - but with good blocking maybe scale's manageable\n",
    "- various commercial tools\n",
    "\n",
    "Be aware of products for special cases!\n",
    "- mailing address deduplication\n",
    "- company name deduplication\n",
    "- person deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Assessing the Results of Entity Resolution\n",
    "\n",
    "Suppose you start with 1 Million records. Your Entity Resolution software comes back with 250K entities. How well did it do?\n",
    "\n",
    "This is a HARD PROBLEM.\n",
    "\n",
    "- Painful to review 250,000 sets and fix by hand!\n",
    "- Maybe the software can prioritize things \"on the bubble\"\n",
    "  - items that have been put together with low confidence\n",
    "  - items that have been separated with low confidence\n",
    "  - requires confidence metric for the distance/clustering\n",
    "- Maybe your fixes reparameterize the ER model\n",
    "- An example of *Active Learning*\n",
    "- Not clear that the user effort is worthwhile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summing Up: Entity Resolution Techniques\n",
    "Basic approach:\n",
    "- Blocking: many approaches (heuristics)\n",
    "- Matching: \n",
    "  - Many distance functions (heuristics)\n",
    "  - Many clustering techniques (heuristics)\n",
    "- Assessing/Fixing results: Hard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summing Up: Entity Resolution in Practice\n",
    "- Entity Resolution is often very important!\n",
    "- Entity Resolution is often rather dodgy \n",
    "  - Heuristics upon heuristics\n",
    "  - Limited human ability to assess results\n",
    "- Quite reliable in specific mature domains\n",
    "  - E.g. mailing address deduplication\n",
    "- Or in very general settings where training data is plentiful\n",
    "  - E.g. NLP\n",
    "- Or when human review is practical\n",
    "  - E.g. crowd-sourceable"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
